{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from working_with_files import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные 1: новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('train.json', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative', 'neutral', 'positive'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple lexicon-based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка и предобработка тонального словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/dkulagin/kartaslov/tree/master/dataset/emo_dict\n",
    "lexicon = pd.read_csv('emo_dict.csv', encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment dict with exact values for pos & neg\n",
    "d = {}\n",
    "for idx, row in lexicon.iterrows():\n",
    "    if row['tag'] != 'NEUT':\n",
    "        d[row['term']] = row['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11324"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация текстов корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import pymorphy2\n",
    "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "        lemmas = [morph_analyzer.parse(word)[0].normal_form for word in words] \n",
    "    except TypeError:\n",
    "        return ''\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_lemmatized'] = data['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для подсчета тональности текста на основе тональности отдельных слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for detecting text's sentiment based on input sentiment dict\n",
    "def detect_sentiment(text, sentiment_dict):\n",
    "    words = text.split()\n",
    "    score = 0\n",
    "    for word in words:\n",
    "        if word in sentiment_dict:\n",
    "            score += sentiment_dict[word]\n",
    "    if score > 1:\n",
    "        return 'positive'\n",
    "    elif score < -1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Алгоритм работает более точно, если пограничными значениями для опредления сентимента являются -1 и 1, а не 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment_by_lex'] = data['text'].apply(detect_sentiment, sentiment_dict=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.46      0.15      0.22      1434\n",
      "     neutral       0.49      0.41      0.45      4034\n",
      "    positive       0.40      0.63      0.49      2795\n",
      "\n",
      "    accuracy                           0.44      8263\n",
      "   macro avg       0.45      0.40      0.39      8263\n",
      "weighted avg       0.45      0.44      0.42      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет сентмента текста с учетом нейтральных слов из словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include neutral values in sentiment dict\n",
    "d1 = {}\n",
    "for idx, row in lexicon.iterrows():\n",
    "    d1[row['term']] = row['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment_by_lex'] = data['text'].apply(detect_sentiment, sentiment_dict=d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.04      0.07      1434\n",
      "     neutral       0.45      0.14      0.21      4034\n",
      "    positive       0.36      0.88      0.51      2795\n",
      "\n",
      "    accuracy                           0.37      8263\n",
      "   macro avg       0.45      0.35      0.26      8263\n",
      "weighted avg       0.43      0.37      0.29      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейтральные слова лучше не учитывать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замена точных значений сентимента в словаре на -1 и 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try sentiment dict w plain +/-1 values\n",
    "d2 = {}\n",
    "for idx, row in lexicon.iterrows():\n",
    "    if row['tag'] == 'NGTV':\n",
    "        d2[row['term']] = -1\n",
    "    if row['tag'] == 'PSTV':\n",
    "        d2[row['term']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment_by_lex'] = data['text'].apply(detect_sentiment, sentiment_dict=d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.14      0.21      1434\n",
      "     neutral       0.50      0.44      0.46      4034\n",
      "    positive       0.40      0.62      0.49      2795\n",
      "\n",
      "    accuracy                           0.45      8263\n",
      "   macro avg       0.45      0.40      0.39      8263\n",
      "weighted avg       0.46      0.45      0.43      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование значений -1 и 1 вместо точных значений дает лучший результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка словаря SentiLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try ru_senti_lex sentiment dict (plain +/-1 values)\n",
    "# source: http://www.labinform.ru/pub/rusentilex/rusentilex_2017.txt\n",
    "d3 = {}\n",
    "with open('ru_senti_lex.csv', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    columns = line.split(',')\n",
    "    if ' ' not in columns[0].strip():\n",
    "        if columns[3].strip() == 'negative':\n",
    "            d3[columns[0].strip()] = -1\n",
    "        if columns[3].strip() == 'positive':\n",
    "            d3[columns[0].strip()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10542"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment_by_lex'] = data['text'].apply(detect_sentiment, sentiment_dict=d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.31      0.35      1434\n",
      "     neutral       0.49      0.64      0.56      4034\n",
      "    positive       0.47      0.32      0.38      2795\n",
      "\n",
      "    accuracy                           0.48      8263\n",
      "   macro avg       0.45      0.42      0.43      8263\n",
      "weighted avg       0.47      0.48      0.46      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь SentiLex лучше первого"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка и предобработка словаря из исследования http://www.dialog-21.ru/media/3402/kotelnikovevetal.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try sentiment dict from here: http://www.dialog-21.ru/media/3402/kotelnikovevetal.pdf\n",
    "# dicts: https://drive.google.com/drive/u/0/folders/0B38i30_htmrTSm1mVXRZNnljNlk\n",
    "# n annotators = 4, pos/neg only\n",
    "\n",
    "dicts = []\n",
    "domains = ['cars', 'movies', 'cameras', 'books', 'restaurants']\n",
    "for domain in domains:\n",
    "    d = {}\n",
    "    with open(domain + '.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            columns = line.split('\\t')\n",
    "            if columns[1] == '4':\n",
    "                d[columns[0]] = 'positive'\n",
    "            if columns[2] == '4':\n",
    "                d[columns[0]] = 'negative'\n",
    "    dicts.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = {}\n",
    "all_words = []\n",
    "for d in dicts:\n",
    "    all_words.extend(list(d.keys()))\n",
    "all_words = set(all_words)\n",
    "\n",
    "for word in all_words:\n",
    "    values = []\n",
    "    for d in dicts:\n",
    "        if word in d.keys():\n",
    "            values.append(d[word])\n",
    "    if len(set(values)) == 1:\n",
    "        d4[word] = list(set(values))[0]\n",
    "        \n",
    "for word, sentiment in d4.items():\n",
    "    if sentiment == 'positive':\n",
    "        d4[word] = 1\n",
    "    else:\n",
    "        d4[word] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.06      0.11      1434\n",
      "     neutral       0.48      0.91      0.63      4034\n",
      "    positive       0.39      0.06      0.11      2795\n",
      "\n",
      "    accuracy                           0.48      8263\n",
      "   macro avg       0.44      0.35      0.28      8263\n",
      "weighted avg       0.44      0.48      0.36      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text'].apply(detect_sentiment, sentiment_dict=d4)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При простом словарном подходе лучший результат показывает словарь SentiLex (d3): accuracy -- 48, F1 -- 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon-based approach w/custom lexicon formed using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from preprocessing import *\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary and train word2vec model\n",
    "def build_w2v_model(documents, min_count):\n",
    "    w2v_model = gensim.models.Word2Vec(documents, size=300, window=5, min_count=min_count)\n",
    "    return w2v_model\n",
    "\n",
    "# tokenize text into list of lists for building w2v model\n",
    "def tokenize_text(text):\n",
    "    stopwords = set(get_lexicon_from_file('ru_stopwords_extended.txt'))\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences_tokenized = []\n",
    "    for sentence in sentences:\n",
    "        words = [token for token in word_tokenize(sentence) if token.isalpha() and token.isascii() == False \n",
    "                 and len(token) > 2 and token not in stopwords]\n",
    "        if words != []:\n",
    "            sentences_tokenized.extend(words)\n",
    "    return sentences_tokenized\n",
    "\n",
    "# check if word in w2v-vocab\n",
    "def check_if_word_in_vocab(model, word):\n",
    "    return word in model.wv.vocab\n",
    "\n",
    "# get semantic associates for a word in a list of w2v models       \n",
    "def get_most_similar(word, topn, model):\n",
    "    try:\n",
    "        print(model.wv.most_similar(positive=word, topn=topn))\n",
    "    except KeyError:\n",
    "        print('\\nNo such word in vocabulary\\n')\n",
    "        \n",
    "def get_most_similar_list(model, word, topn):\n",
    "    return list(dict(model.wv.most_similar(positive=word, topn=topn)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8263"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for idx, row in data.iterrows():\n",
    "    corpus.append(tokenize_text(row['text_lemmatized']))\n",
    "    \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение word2vec-модели на текстах корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = build_w2v_model(corpus, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формирование расширенного тонального словаря на основе семантических ассоциатов каждого слова в изначальном словаре\n",
    "\n",
    "Референсное исследование: https://cyberleninka.ru/article/n/analiz-primeneniya-distributivno-semanticheskih-modeley-dlya-popolneniya-slovarya-otsenochnoy-leksiki/viewer\n",
    "\n",
    "Изначальный словарь -- самый маленький из использованных выше (1114 слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d5 = d4.copy()\n",
    "\n",
    "for word in d4.keys():\n",
    "    try: \n",
    "        associates = get_most_similar_list(w2v_model, word, 10)\n",
    "        for associate in associates:\n",
    "            d5[associate] = d5[word]\n",
    "    except KeyError:\n",
    "        pass   \n",
    "    \n",
    "len(d5) # +1638 new words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сентимент-анализ с использованием расширенного словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.37      0.39      1434\n",
      "     neutral       0.51      0.29      0.37      4034\n",
      "    positive       0.42      0.69      0.52      2795\n",
      "\n",
      "    accuracy                           0.44      8263\n",
      "   macro avg       0.44      0.45      0.43      8263\n",
      "weighted avg       0.46      0.44      0.42      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text_lemmatized'].apply(detect_sentiment, sentiment_dict=d5)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сравнению с простым словарным подходом результат не стал лучше. F-мера для того же словаря повысилась (42 против 36), но по простой SentiLex все равно показывает более качественный результат -- 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширение словаря не 10, а 5 ближайшими семантическими ассоциатами каждого слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2030"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 closest associates\n",
    "\n",
    "d6 = d4.copy()\n",
    "for word in d4.keys():\n",
    "    try: \n",
    "        associates = get_most_similar_list(w2v_model, word, 5)\n",
    "        for associate in associates:\n",
    "            d6[associate] = d6[word]\n",
    "    except KeyError:\n",
    "        pass    \n",
    "    \n",
    "len(d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.31      0.34      1434\n",
      "     neutral       0.51      0.41      0.45      4034\n",
      "    positive       0.41      0.57      0.47      2795\n",
      "\n",
      "    accuracy                           0.44      8263\n",
      "   macro avg       0.43      0.43      0.42      8263\n",
      "weighted avg       0.45      0.44      0.44      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text_lemmatized'].apply(detect_sentiment, sentiment_dict=d6)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшить результат снова не удалось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем взять словарь SentiLex и расширить его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15626"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d7 = d3.copy()\n",
    "\n",
    "# 10 closest associates\n",
    "for word in d3.keys():\n",
    "    try: \n",
    "        associates = get_most_similar_list(w2v_model, word, 10)\n",
    "        for associate in associates:\n",
    "            d7[associate] = d7[word]\n",
    "    except KeyError:\n",
    "        pass   \n",
    "    \n",
    "len(d7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.57      0.43      1434\n",
      "     neutral       0.48      0.09      0.16      4034\n",
      "    positive       0.41      0.75      0.53      2795\n",
      "\n",
      "    accuracy                           0.40      8263\n",
      "   macro avg       0.41      0.47      0.37      8263\n",
      "weighted avg       0.43      0.40      0.33      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text_lemmatized'].apply(detect_sentiment, sentiment_dict=d7)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-мера упала, лучше брать словарь поменьше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем использовать word2vec-модель, обученную не на самих классифицируемых текстах, а на более крупном корпусе новостей (источник: RusVectores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model1 = KeyedVectors.load_word2vec_format('russian_news_model_rusvectores.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составление тонального словаря с предварительной очисткой слов от pos-тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTJ', 'VERB', 'PROPN', 'SYM', 'NUM', 'NOUN', 'ADV', 'ADJ', 'X']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = []\n",
    "for word in w2v_model1.wv.vocab:\n",
    "    pos_tag = word.split('_')[1]\n",
    "    pos_tags.append(pos_tag)\n",
    "pos_tags = list(set(pos_tags))\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5644"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d8 = d4.copy()\n",
    "\n",
    "for word in d4.keys():\n",
    "    for pos_tag in pos_tags:\n",
    "        try: \n",
    "            associates = get_most_similar_list(w2v_model1, word + '_' + pos_tag, 10)\n",
    "            for associate in associates:\n",
    "                associate_clean = associate.split('_')[0]\n",
    "                if associate_clean.isascii() == False and associate_clean.isalpha():\n",
    "                    d8[associate_clean] = d8[word]\n",
    "            break\n",
    "        except KeyError:\n",
    "            pass   \n",
    "        \n",
    "len(d8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.24      0.24      0.24      1434\n",
      "     neutral       0.48      0.69      0.56      4034\n",
      "    positive       0.38      0.13      0.20      2795\n",
      "\n",
      "    accuracy                           0.42      8263\n",
      "   macro avg       0.37      0.35      0.33      8263\n",
      "weighted avg       0.40      0.42      0.38      8263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text'].apply(detect_sentiment, sentiment_dict=d8)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ощутимого улучшения результата нет (как и при использовании большего или меньшего количества семантических ассоциатов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные 2: отзывы о кино"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reviews_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46501"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) # Корпус гораздо больше первого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 0, 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование числовых значений сентимента в строковые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_num_to_word(sentiment):\n",
    "    if sentiment == -1:\n",
    "        return 'negative'\n",
    "    if sentiment == 0:\n",
    "        return 'neutral'\n",
    "    if sentiment == 1:\n",
    "        return 'positive'\n",
    "    \n",
    "data['sentiment'] = data['label'].apply(sentiment_num_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple lexicon-based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты сентимент-анализа с использованием разных словарей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.19      0.35      0.25      4376\n",
      "     neutral       0.14      0.37      0.20      5645\n",
      "    positive       0.85      0.54      0.66     36480\n",
      "\n",
      "    accuracy                           0.50     46501\n",
      "   macro avg       0.39      0.42      0.37     46501\n",
      "weighted avg       0.70      0.50      0.57     46501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text'].apply(detect_sentiment, sentiment_dict=d3)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маленький словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.22      0.29      4376\n",
      "     neutral       0.14      0.57      0.23      5645\n",
      "    positive       0.87      0.51      0.64     36480\n",
      "\n",
      "    accuracy                           0.49     46501\n",
      "   macro avg       0.47      0.43      0.39     46501\n",
      "weighted avg       0.74      0.49      0.56     46501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text'].apply(detect_sentiment, sentiment_dict=d4)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты примерно одинаковые, у SentiLex чуть выше: accuracy -- 50, F-мера -- 57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon-based approach w/custom lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение word2vec-модели на самом корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46501"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for idx, row in data.iterrows():\n",
    "    corpus.append(tokenize_text(row['text_lemmatized']))\n",
    "    \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model2 = build_w2v_model(corpus, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширение тонального словаря, 10 семантических ассоциатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5447"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d9 = d4.copy()\n",
    "for word in d4.keys():\n",
    "    try: \n",
    "        associates = get_most_similar_list(w2v_model2, word, 10)\n",
    "        for associate in associates:\n",
    "            d9[associate] = d9[word]\n",
    "    except KeyError:\n",
    "        pass   \n",
    "len(d9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.27      0.39      0.32      4376\n",
      "     neutral       0.16      0.14      0.15      5645\n",
      "    positive       0.84      0.81      0.83     36480\n",
      "\n",
      "    accuracy                           0.69     46501\n",
      "   macro avg       0.42      0.45      0.43     46501\n",
      "weighted avg       0.70      0.69      0.70     46501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text_lemmatized'].apply(detect_sentiment, sentiment_dict=d9)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат заметно лучше. По сравнению с маленьким словарем F-мера выросла с 56 до 70, точность -- с 49 до 69 (у простого SentiLex F-мера была 57, точность -- 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем взять 5 семантических ассоциатов вместо 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3556"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d10 = d4.copy()\n",
    "for word in d4.keys():\n",
    "    try: \n",
    "        associates = get_most_similar_list(w2v_model2, word, 5)\n",
    "        for associate in associates:\n",
    "            d10[associate] = d10[word]\n",
    "    except KeyError:\n",
    "        pass   \n",
    "len(d10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.41      0.34      4376\n",
      "     neutral       0.16      0.18      0.17      5645\n",
      "    positive       0.85      0.79      0.82     36480\n",
      "\n",
      "    accuracy                           0.68     46501\n",
      "   macro avg       0.43      0.46      0.44     46501\n",
      "weighted avg       0.71      0.68      0.70     46501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text_lemmatized'].apply(detect_sentiment, sentiment_dict=d10)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С 10 результат чуть лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7019"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d11 = d4.copy()\n",
    "for word in d4.keys():\n",
    "    try: \n",
    "        associates = get_most_similar_list(w2v_model2, word, 15)\n",
    "        for associate in associates:\n",
    "            d11[associate] = d11[word]\n",
    "    except KeyError:\n",
    "        pass   \n",
    "len(d11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 ассоциатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.25      0.44      0.32      4376\n",
      "     neutral       0.16      0.14      0.15      5645\n",
      "    positive       0.84      0.79      0.82     36480\n",
      "\n",
      "    accuracy                           0.68     46501\n",
      "   macro avg       0.42      0.46      0.43     46501\n",
      "weighted avg       0.71      0.68      0.69     46501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text_lemmatized'].apply(detect_sentiment, sentiment_dict=d11)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимальное число ассоциатов -- 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем модель из похожего домена с RusVectores -- web_mystem_skipgram_500_2_2015\n",
    "\n",
    "Источник: https://rusvectores.org/ru/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model3 = KeyedVectors.load_word2vec_format('web.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5496"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = []\n",
    "for word in w2v_model3.wv.vocab:\n",
    "    pos_tag = word.split('_')[1]\n",
    "    pos_tags.append(pos_tag)\n",
    "pos_tags = list(set(pos_tags))\n",
    "\n",
    "d12 = d4.copy()\n",
    "\n",
    "for word in d4.keys():\n",
    "    for pos_tag in pos_tags:\n",
    "        try: \n",
    "            associates = get_most_similar_list(w2v_model3, word + '_' + pos_tag, 10)\n",
    "            for associate in associates:\n",
    "                associate_clean = associate.split('_')[0]\n",
    "                if associate_clean.isascii() == False and associate_clean.isalpha():\n",
    "                    d12[associate_clean] = d12[word]\n",
    "            break\n",
    "        except KeyError:\n",
    "            pass   \n",
    "        \n",
    "len(d12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.18      0.62      0.28      4376\n",
      "     neutral       0.13      0.14      0.14      5645\n",
      "    positive       0.87      0.60      0.71     36480\n",
      "\n",
      "    accuracy                           0.55     46501\n",
      "   macro avg       0.39      0.45      0.37     46501\n",
      "weighted avg       0.71      0.55      0.60     46501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['sentiment_by_lex'] = data['text_lemmatized'].apply(detect_sentiment, sentiment_dict=d12)\n",
    "print(classification_report(data.sentiment, data.sentiment_by_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели на самом корпусе дает лучшие результаты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
